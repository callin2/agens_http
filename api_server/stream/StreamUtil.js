// Generated by CoffeeScript 2.3.1
// StreamUtil
// ==========

// > Agens Flow에서 기본적으로 제공하는 StreamTask들과 그에따른 로직을 포함한다.

// 파일시스템관련 라이브러리
var Chunk, MongoClient, Readable, Writable, ag, aux, batch, counter, dataMap, debug, drop, fromJSON, fromMongo, fromMysql, fromUrl, fs, intoFile, intoMongo, map, moving, mysql, path, progress, pumpify, qw, reduce, sampling, step, take, tap, tap2, tapIntoAgens, tapIntoMysql, throttle, through2, through2Concurrent, toChunk, toString, util, withErrorBypass, yajs;

fs = require('fs');

path = require('path');

// 스트림 관련 라이브러리
through2 = require('through2');

pumpify = require('pumpify');

({Readable, Writable} = require('stream'));

through2Concurrent = require('through2-concurrent');

// Promise 관련 라이브러리
util = require('util');

// 데이타베이스 관련 라이브러리
ag = require('agensgraph');

MongoClient = require('mongodb').MongoClient;

mysql = require('mysql');

({qw} = require('../postgres/util.js'));

// Json 관련 라이브러리
yajs = require('yajson-stream');

// 스트림에 공통으로 흘러가는 데이타 모델
Chunk = require('./Chunk');

// ## withErrorBypass
// transform function `xf`를 error chunk일 경우 아무일도 하지 않는 transform function으로 변환한다.
withErrorBypass = function(xfFnc) {
  return function(chunk, encoding, next) {
    if (chunk.error) {
      return next(null, chunk);
    }
    return xfFnc.call(this, chunk, encoding, next);
  };
};

// ## progress
// 어느정도 처리했는지 처리상태를 모니터링하기위한 보조 스트림이다.
// `interval` $_{ms}$ 마다 진행정보를 처리중인 chunk의 `meta.progress.transferred` 에 기록하여 스트림에 흘려보낸다.
// 전체건수를 알 수 있으면 `meta.progress.remaining` 도 알 수 있는데 전체건수는 chunk의 `control.progress.length` 를 넣어주면
// 그 이후로 남은건수 `meta.progress.remaining` 도  보내준다.
progress = (options) => {
  var delta, doControlProcess, flush, interval, lastMeta, length, nextUpdate, progress_meta, setLength, startTime, tr, transferred, xf;
  transferred = 0;
  delta = 0;
  length = options.length || 0;
  interval = options.time || 1000;
  startTime = Date.now();
  nextUpdate = startTime + interval;
  progress_meta = {
    transferred,
    length,
    startTime,
    now: startTime,
    speed: 0
  };
  lastMeta = null;
  // 제어신호에서 전체 데이타의 크기 `length` 를 전달받아 남은건수 계산에 사용한다.
  doControlProcess = (control) => {
    console.log('progress control', control);
    return setLength(control.length);
  };
  setLength = (newLength) => {
    length = newLength;
    progress_meta.length = length;
    return progress_meta.remaining = length - transferred;
  };
  xf = function(chunk, encoding, next) {
    var ref;
    transferred++;
    delta++;
    if (chunk.control && chunk.control.progress) {
      // chunk에 control 속성이 들어있고 그중에 `progress` 속성이 있는경우 여기서 처리해야할 제어신호가 들어온 것이다.
      // 제어신호처리로직인 `doControlProcess`를 호출한다.
      doControlProcess(chunk.control.progress);
    }
    lastMeta = chunk.meta;
    progress_meta.transferred = transferred;
    progress_meta.remaining = (ref = length >= transferred) != null ? ref : length - {
      transferred: 0
    };
    // interval 이 경과하여 이번에 지나가는 chunk의 meta에 progress정보를 넣어준다.
    // `meta.debug` 를 `true` 로 해야 최종적으로 client에 websocket으로 메세지가 전달된다.
    if (Date.now() > nextUpdate) {
      chunk.meta.progress = progress_meta;
      chunk.meta.debug = true;
      nextUpdate = Date.now() + interval;
    }
    return next(null, chunk);
  };
  // 모든 데이타를 다 보내고 마지막으로 progress 정보를 보낸다.
  // `lastMeta` 에는 flowID 가 들어있기 때문에 그냥 만들지 않고 저장해두었다가 사용한다.
  flush = function(cb) {
    var c;
    console.log('flush from progress');
    c = new Chunk({
      data: null
    });
    c.meta = lastMeta;
    c.meta.progress = progress_meta;
    c.meta.debug = true;
    this.push(c);
    return cb();
  };
  //무조건 objectMode
  tr = through2.obj({
    highWaterMark: 1
  }, withErrorBypass(xf), flush);
  tr.setLength = setLength;
  return tr;
};

progress._meta = {
  type: 'transform',
  name: 'progress',
  option: [
    {
      optionName: 'time',
      type: 'number',
      default: 1000
    },
    {
      optionName: 'length',
      type: 'number',
      default: 0
    }
  ]
};

// 정해진 갯수만큼만  chunk 를 흘려보낸다. 갯수를 넘거나 더이상 chunk가 없을경우 종료한다.
take = function({cnt}) {
  var idx;
  cnt = +cnt;
  if (!(typeof cnt === 'number' && cnt > 0)) {
    throw new Error('invalid limit count.' + cnt);
  }
  idx = 0;
  return through2.obj({
    highWaterMark: 1
  }, withErrorBypass(function(chunk, encoding, next) {
    if (idx >= cnt) {
      this.push(null);
    } else {
      next(null, chunk);
    }
    return idx++;
  }));
};

take._meta = {
  type: 'transform',
  name: 'take',
  option: [
    {
      optionName: 'cnt',
      type: 'number',
      default: 10
    }
  ]
};

// /**
// * tick()을 호출할때 한개씩 stream으로 내보낸다.
// * @returns {*}
// */
step = function() {
  var _store, stream, xf;
  _store = {
    buffer: [],
    isEnd: false
  };
  xf = function(chunk, encoding, callback) {
    return _store.buffer.push(() => {
      return callback(null, chunk);
    });
  };
  stream = through2.obj({
    highWaterMark: 1
  }, withErrorBypass(xf));
  stream.tick = () => {
    if (_store.buffer.length > 0) {
      return _store.buffer.shift()();
    } else if (_store.isEnd) {
      return true;
    }
  };
  stream.on('end', () => {
    return _store.isEnd = true;
  });
  return stream;
};

// 정해진 갯수만큼 object를 버리고 그 다음부터 스트림에 기록
drop = function({
    cnt: count
  }) {
  var idx;
  if (count <= 0) {
    throw new Error('invalid drop count.' + count);
  }
  idx = 0;
  return through2.obj(function(chunk, encoding, callback) {
    if (idx < count) {
      idx++;
      return callback();
    } else {
      return callback(null, chunk);
    }
  });
};

drop._meta = {
  type: 'transform',
  name: 'drop',
  option: [
    {
      optionName: 'count',
      type: 'number',
      default: 10
    }
  ]
};

tap = function({fnc}) {
  var f;
  f = null;
  if (typeof arguments[0] === 'function' && !fnc) {
    f = arguments[0];
  } else if (typeof fnc === 'function') {
    f = fnc;
  } else if (typeof fnc === 'string') {
    f = new Function('data', fnc);
  } else {
    throw new Error('invalid paramter', arguments);
  }
  return through2.obj(function(chunk, encoding, callback) {
    f(chunk);
    return callback(null, chunk);
  }, (cb) => {
    console.log('>>>> flush from tap');
    return cb();
  });
};

// /**
// *
// * @param factoryFunc  (readableSteam) ->
// * @returns {*}
// */
tap2 = function(factoryFunc) {
  tap = new Readable({
    read: function() {},
    objectMode: true
  });
  factoryFunc(tap);
  return through2.obj(function(chunk, encoding, next) {
    console.log('chunk', chunk);
    tap.push(chunk);
    return next(null, chunk);
  }, (cb) => {
    console.log('flush from tap2');
    tap.push(null);
    return cb();
  });
};

debug = function({label, includeHappyCase, toTheServerConsole} = {
    label: '',
    includeHappyCase: false,
    toTheServerConsole: false
  }) {
  return through2.obj(function(chunk, encoding, callback) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    if (chunk.error) {
      chunk.meta.debug = true;
      if (toTheServerConsole) {
        console.error(`[${label}]`, chunk.error, chunk.data);
      }
    } else {
      if (includeHappyCase) {
        chunk.meta.debug = true;
        if (toTheServerConsole) {
          console.info(`[${label}]`, chunk.data);
        }
      }
    }
    return callback(null, chunk);
  });
};

debug._meta = {
  type: 'transform',
  name: 'debug',
  option: [
    {
      optionName: 'label',
      type: 'string',
      default: 'debug label'
    },
    {
      optionName: 'includeHappyCase',
      type: 'boolean',
      default: false
    },
    {
      optionName: 'toTheServerConsole',
      type: 'boolean',
      default: false
    }
  ]
};

// ## sampling
// 발생하는 모든 데이타를 처리할 필요가 없거나 너무 많아서 표본 일부만 발췌해서 처리할 필요가 있을경우
// `period`주가마다 한개씩 chunk 표본을 추출하는 SrtreamTask
sampling = function({period}) {
  var _latestEmitTime;
  _latestEmitTime = Date.now() - period;
  return through2.obj(function(chunk, encoding, callback) {
    var now;
    now = Date.now();
    if ((now - _latestEmitTime) > period) {
      callback(null, chunk);
      return _latestEmitTime = now;
    } else {
      return callback();
    }
  });
};

sampling._meta = {
  type: 'transform',
  name: 'sampling',
  option: [
    {
      optionName: 'period',
      type: 'number',
      default: 100
    }
  ]
};

map = function({fnc}) {
  var f;
  f = (typeof arguments[0] === 'function' && !fnc) ? arguments[0] : new Function('data', fnc);
  return through2.obj(withErrorBypass(function(chunk, encoding, callback) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    return callback(null, chunk.map(f));
  }));
};

map._meta = {
  type: 'transform',
  name: 'map',
  option: [
    {
      optionName: 'fnc',
      type: 'function',
      default: "return [\n    data.id,\n    data.recipOrgName,\n    data.fundingOrgName,\n    data.currency\n]"
    }
  ]
};

// 이동평균을 구한다. 이동평균을 구하는 로직은 `fnc` function에 있어야 한다.
moving = function({fnc, init}) {
  var movingAggrValue;
  movingAggrValue = new Chunk(init);
  return through2.obj(function(chunk, encoding, next) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    if (!chunk.error) {
      movingAggrValue = new Chunk(fnc(movingAggrValue.data, chunk.data));
    } else {

    }
    // bypass error 에러로 결과를 바꿔야 하나 아니면 에러난건 빼고 지나쳐야 하나?
    // movingAggrValue = chunk
    return next(null, movingAggrValue);
  });
};

// // moving._meta = {
// //     type: 'transform',
// //     name: 'moving',
// //     option: [
// //         {optionName:"fnc", type: "number", default: (d)=>d},
// //         {optionName:"init", type: "number", default: (d)=>d},
// //         { period: "number"}
// //     ]
// // };
reduce = function(fnc, init) {
  var aggrValue;
  aggrValue = new Chunk(init);
  return through2.obj(function(chunk, encoding, next) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    if (!chunk.error) {
      aggrValue = new Chunk(fnc(aggrValue.data, chunk.data));
    } else {

    }
    // bypass error 에러로 결과를 바꿔야 하나 아니면 에러난건 빼고 지나쳐야 하나?
    // aggrValue = chunk
    return next();
  }, cb(() => {
    return cb(null, aggrValue);
  }));
};

// ## throttle
// dur millisec 마다 1개의 chunk를 emit 한다. `sampling` 은 주기가 도래하지 않았을때 흘러들어온 chunk를 버리지만
// `throttle`은 모든 chunk를 처리한다.
throttle = function({dur}) {
  var flush, handle, pendingChunk, pendingNext, stream, xf;
  pendingChunk = null;
  pendingNext = null;
  xf = function(chunk, encoding, next) {
    if (pendingChunk) {
      throw new Error('remain untransformed prev chunk');
    }
    pendingChunk = chunk;
    return pendingNext = next;
  };
  handle = setInterval(() => {
    var _pchunk, _pnext;
    if (pendingChunk) {
      _pnext = pendingNext;
      _pchunk = pendingChunk;
      Promise.resolve().then(() => {
        return _pnext(null, _pchunk);
      });
      pendingChunk = null;
      return pendingNext = null;
    }
  }, dur);
  flush = function(callback) {
    console.log('clear Interval');
    clearInterval(handle);
    return callback();
  };
  stream = through2.obj({
    highWaterMark: 1
  }, withErrorBypass(xf), flush);
  return stream;
};

throttle._meta = {
  type: 'transform',
  name: 'throttle',
  option: [
    {
      optionName: 'dur',
      type: 'number',
      default: 1000
    }
  ]
};

counter = function() {
  var index;
  index = 0;
  return map((d) => {
    return {
      count: ++index,
      data: d
    };
  });
};

// ## batch
// 정해진 버퍼 사이즈만큼 데이타를 모아서 한번에 보내주는 스트림
// 이 StreamTask를 통과한 chunk의 data 는 항상 `Array` type 이다.
batch = function({size}) {
  var _internalBuffer;
  if (size <= 1) {
    throw new Error('invalid bulk size.' + size);
  }
  _internalBuffer = null; //new Chunk({data:[]})
  return through2.obj({
    highWaterMark: 1
  }, withErrorBypass(function(chunk, encoding, next) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    if (!_internalBuffer) {
      _internalBuffer = new Chunk({
        data: []
      });
      _internalBuffer.meta = chunk.meta;
      _internalBuffer.control = _internalBuffer.control && chunk.control;
    }
    _internalBuffer.data.push(chunk.data);
    if (_internalBuffer.data.length === size) {
      this.push(_internalBuffer);
      _internalBuffer = null;
    }
    return next();
  }, function(callback) { // flush function
    this.push(_internalBuffer);
    _internalBuffer = null;
    return callback();
  }));
};

batch._meta = {
  type: 'transform',
  name: 'batch',
  option: [
    {
      optionName: 'size',
      type: 'number',
      default: 10
    }
  ]
};

// /**
// * @param rule
// * key value pair
// * key :  'column name' | 'attribute path' of target database
// * value :  'column name' | 'attribute path' | xFnc  of source database
// */
dataMap = function({rule}) {
  var _compiledRule, xf;
  _compiledRule = null;
  if (rule instanceof Array) {
    _compiledRule = rule.map((r) => {
      if (typeof r === 'string') {
        return (obj) => {
          return obj[r];
        };
      } else if (typeof r === 'function') {
        return r;
      } else if (r instanceof Array) { // object key path array
        return (obj) => {
          return r.reduce((result, item) => {
            return result[item];
          }, obj);
        };
      } else {
        throw new Error('rule  error ' + rule);
      }
    });
  } else if (typeof rule === 'object') {
    _compiledRule = {};
    Object.keys(rule).forEach((k) => {
      if (typeof rule[k] === 'string') {
        return _compiledRule[k] = obj(() => {
          return obj[rule[k]];
        });
      } else if (typeof rule[k] === 'function') {
        return _compiledRule[k] = rule[k];
      } else if (rule[k] instanceof Array) {
        return _compiledRule[k] = (obj) => {
          return rule[k].reduce((result, item) => {
            return result[item];
          }, obj);
        };
      } else {
        throw new Error('rule  error ' + rule);
      }
    });
  }
  xf = function(chunk, encoding, next) {
    var result;
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    result = null;
    if (chunk.error) {
      result = chunk;
    } else {
      if (rule instanceof Array) {
        result = new Chunk([]);
        result.data = _compiledRule.map((rule) => {
          var e;
          try {
            return rule(chunk.data);
          } catch (error1) {
            e = error1;
            return result.error = e;
          }
        }, result);
      } else {
        result = new Chunk({});
        Object.keys(rule).reduce((r, k) => {
          var cr;
          cr = chunk.map(_compiledRule[k]);
          r[k] = cr.data;
          result.error = result.error || cr.error;
          return r;
        }, result.data);
      }
    }
    return next(null, result);
  };
  return through2.obj({
    highWaterMark: 1
  }, xf, cb(() => {
    console.log('flush from dataMap');
    _compiledRule = null;
    return cb();
  }));
};

// // dataMap._meta = {
// //     type: 'transform',
// //     name: 'dataMap',
// //     option: [
// //         {optionName:"rule", type: "object|array", default:["id",
// //                 ["recipientOrganization",0,"name"] ,
// //                 (obj)=>obj.fundingOrganization[0].name ,
// //                 "currency"]},
// //     ],
// //     optionExample: {
// //         rule     : [
// //             "id",
// //             ["recipientOrganization",0,"name"] ,
// //             (obj)=>obj.fundingOrganization[0].name ,
// //             "currency"
// //         ]
// //     }
// // };
tapIntoMysql = function(query, {host, port, user, password, database, concurrent, batchMode}) {
  var concrnt, connection, flush, pquery, stream, xf;
  concrnt = concurrent || 10;
  connection = mysql.createPool({
    host,
    port,
    user,
    password,
    database,
    connectionLimit: concrnt
  });
  pquery = util.promisify(connection.query.bind(connection));
  xf = function(chunk, encoding, callback) {
    var d;
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    d = batchMode ? [chunk] : chunk;
    return connection.query(query, d, async(err, rows, fields) => {
      var i, j, len, ref;
      if (!err) {
        return callback();
      } else {
        if (batchMode) {
          ref = [0, chunk.length - 1];
          for (j = 0, len = ref.length; j < len; j++) {
            i = ref[j];
            try {
              await pquery(query, [[chunk[i]]]);
            } catch (error1) {
              err = error1;
              this.push({
                err,
                data: chunk[i],
                query
              });
            }
          }
          return callback();
        } else {
          return callback(null, {err, chunk, query});
        }
      }
    });
  };
  flush = function(cb) {
    console.log('close connection ');
    connection.end();
    return cb();
  };
  stream = through2Concurrent.obj({
    maxConcurrency: concrnt
  }, xf, flush);
  stream.on('pipe', (src_stream) => {});
  // todo : data shape check here
  // console.log("src_stream", src_stream)
  return stream;
};

// /**
// *
// * @param {Object} option
// * @param {(string|Object)} option.query  tableName or { count, select }  countQuery selectQuery
// * @param {string} option.query.count   query that return total row count
// * @param {string} option.query.select  select query
// * @param {string} option.host host domain or ip address
// * @param {number} option.port port number
// * @param {string} option.user user id
// * @param {string} option.password user password
// * @param {string} option.database database name
// * @returns {Object} {Fg,Bg}
// */
fromMysql = function(option) {
  var _option, _q, _rowCount, connection, database, host, lenStream, password, port, query, stream, user;
  _option = Object.assign({}, fromMysqlDefaultOption, option);
  ({host, port, user, password, database, query} = _option);
  _rowCount = -1;
  _q = (typeof query === 'string') ? {
    count: `select count(*) as cnt from ${query}`,
    select: `select * from ${query}`
  } : query;
  connection = mysql.createConnection({host, port, user, password, database});
  //  get total size of data
  connection.query(_q.count, (error, results) => {
    if (!error) {
      return _rowCount = results[0].cnt;
    } else {
      return console.error(error);
    }
  });
  stream = connection.query(_q.select).stream({
    highWaterMark: 1
  });
  stream.on('end', () => {
    console.log('fromMysql close connection');
    return connection.end();
  });
  // table 의 row 카운트를 control신호에 흘려 보내고 나머지 모든 chunk를 그대로 보내는 스트림
  lenStream = through2.obj(function(chunk, encoding, callback) {
    var c;
    if (_rowCount !== -1) {
      c = new Chunk({
        control: {
          progress: {
            length: _rowCount
          }
        }
      });
      _rowCount = -1;
      this.push(c);
      return runAtNextTick(() => {
        return callback(null, chunk);
      });
    }
  });
  return stream.pipe(lenStream);
};

fromMysql._meta = {
  type: 'source',
  name: 'fromMysql',
  option: [
    {
      optionName: 'query',
      type: 'string',
      default: 'grants_t01'
    },
    {
      optionName: 'host',
      type: 'string',
      default: 'localhost'
    },
    {
      optionName: 'port',
      type: 'number',
      default: 3306
    },
    {
      optionName: 'user',
      type: 'string',
      default: 'root'
    },
    {
      optionName: 'password',
      type: 'string',
      default: 'my-secret-pw'
    },
    {
      optionName: 'database',
      type: 'string',
      default: 'giving360'
    }
  ],
  optionExample: {
    host: 'localhost',
    user: 'root',
    password: 'my-secret-pw',
    database: 'giving360',
    query: 'grants_t01'
  }
};

// Agens Graph에 데이타를 로딩하는 StreamTask
// 쿼리 파라미터는 Array로 전달해야 한다. 쿼리가 아래와 같다면
// ```sql
// create (:grants_ext { _id:$1  , recipOrgName:$2, fundingOrgName:$3, currency: $4 } );
// ```
// `chunk.data`  에 값이 4개가 들어 있어야 한다.
// ```javascript
// chunk.data = ['id_1', 'org 01', 'org 0001', '$']
// ```

// `batchMode` 가 true인 경우 `chunk.data` 는 아래처럼 Array 의 Array 가 들어와야 한다.
// ```javascript
//     chunk.data = [
//         ['id_1', 'org 01', 'org 0001', '$'],
//         ['id_2', 'org 02', 'org 0002', '$'],
//         ['id_3', 'org 03', 'org 0003', '$']
//     ]
// ```
tapIntoAgens = function(option) {
  var _pipeList, batchMode, concrnt, concurrent, database, flush, graphPath, host, password, pool, port, progress_steam, query, stream, user, xf;
  ({query, host, port, user, password, database, graphPath, concurrent, batchMode} = option);
  concrnt = concurrent || 4;
  // `xf`에서 사용할 pool  `xf`는 새로운 chunk가 발생할때마다 호출되므로 바깥쪽에 선언한다.
  pool = new ag.Pool({host, port, user, password, database});
  // agens graph 에서 cypher query 를 수행하려면 `graph_path`가 필수이다. 그래서 pool에서 connection을 할때마다 `graph_path`를 설정한다.
  pool.on('connect', (client) => {
    return client.query(`SET graph_path = ${graphPath} `);
  });
  // chunk의 data를 agens graph 에 넣는일을 하는 function
  xf = function(chunk, encoding, next) {
    var _this;
    if (chunk.error) {
      return next(null, chunk);
    }
    _this = this;
    // `batchMode`일 경우
    if (batchMode) {
      return pool.connect((err, client, done) => {
        return client.query('BEGIN;').then(() => {
          var count, datalist;
          datalist = chunk.data;
          count = datalist.length;
          return Promise.all(datalist.map((d) => {
            return client.query(qw(query, d)).then((r) => {
              return new Chunk(JSON.stringify(r));
            }).catch((e) => {
              return new Chunk(new Error('data: ' + JSON.stringify(d)));
            });
          })).then((qr) => {
            qr.forEach((_r) => {
              return _this.push(_r);
            });
            client.query('COMMIT;');
            done();
            return next(null);
          }).catch((err) => {
            client.query('ROLLBACK;');
            done();
            return next(null, new Chunk({
              error: err
            }));
          });
        });
      });
    } else {
      // `batchMode`가 아닐경우 건건히 처리한다.
      return pool.query(qw(query, chunk.data), (err, resp) => {
        if (!err) {
          return next(null, new Chunk(`${resp.command} ${resp.rowCount} row OK data=[` + chunk.data(+']')));
        } else {
          // console.error(err)
          // err.message += (" " + err.detail + " data=[" + chunk.data + "]")
          chunk.error = err.message + '\n' + err.stack;
          return next(null, chunk);
        }
      });
    }
  };
  flush = function(cb) {
    console.log('close connection agens');
    // todo REINDEX VLABEL test_vlabel;

    // 아래문장 주석풀면 일부 데이타 누락되는 현상이 있음.
    // pool.end();
    return cb();
  };
  _pipeList = [];
  if (option.progress) {
    progress_steam = progress({
      time: 500
    });
    progress_steam.on('progress', (c) => {
      return console.log('Agens progress', c.speed + ' items/s ' + c.runtime);
    });
    progress_steam.on('end', (c) => {
      return console.log('Agens progress end', c);
    });
    _pipeList.push(progress_steam);
  }
  stream = through2Concurrent.obj({
    highWaterMark: 1,
    maxConcurrency: concrnt
  }, withErrorBypass(xf), flush);
  if (_pipeList.length < 1) {
    return stream;
  } else {
    _pipeList.push(stream);
  }
  return pumpify.obj(_pipeList);
};

tapIntoAgens._meta = {
  type: 'transform',
  name: 'tapIntoAgens',
  option: [
    {
      optionName: 'query',
      type: 'string',
      default: '  create (:grants_ext { _id:$1  , recipOrgName:$2, fundingOrgName:$3, currency: $4 } );  '
    },
    {
      optionName: 'host',
      type: 'string',
      default: 'localhost'
    },
    {
      optionName: 'port',
      type: 'number',
      default: '5532'
    },
    {
      optionName: 'user',
      type: 'string',
      default: 'agens'
    },
    {
      optionName: 'password',
      type: 'string',
      default: 'agens'
    },
    {
      optionName: 'database',
      type: 'string',
      default: 'agens'
    },
    {
      optionName: 'graphPath',
      type: 'string',
      default: 'network'
    },
    {
      optionName: 'concurrent',
      type: 'number',
      default: 4
    },
    {
      optionName: 'batchMode',
      type: 'boolean',
      default: true
    }
  ]
};

// 사용할때 반드시 call 하세요 이렇게  `aux()`
aux = function() {
  return new Writable({
    objectMode: true,
    write: function(a, b, next) {
      return next();
    }
  });
};

fromJSON = function(option) {
  var files, stream, targetJson;
  if (!option.filePath || option.filePath === '') {
    files = fs.readdirSync(global.etl_dataDirectory);
    throw new Error('option.filePath required! \n' + 'possible fileNames are \n ' + files.join('\n'));
  }
  // =================== end of check pre condition
  targetJson = path.resolve(global.etl_dataDirectory, option.filePath);
  stream = fs.createReadStream(targetJson);
  return stream.pipe(yajs(option.selector || '$'));
};

fromJSON._meta = {
  type: 'source',
  name: 'fromJSON',
  option: [
    {
      optionName: 'filePath',
      type: 'string',
      default: 'snp500.json'
    },
    {
      optionName: 'selector',
      type: 'string',
      default: '$'
    }
  ]
};

toChunk = function(flowId) {
  if (!flowId) {
    throw new Error('unkown flowId');
  }
  return through2.obj({
    highWaterMark: 1
  }, function(chunk, encoding, next) {
    if (chunk instanceof Chunk) {
      return next(null, chunk);
    } else {
      return next(null, new Chunk({
        data: chunk
      }, flowId));
    }
  });
};

toString = function() {
  return map((d) => {
    return JSON.stringify(d);
  });
};

intoFile = function({filePath}) {
  var flush, stream, ws, xf;
  ws = fs.createWriteStream(filePath);
  xf = function(chunk, enc, next) {
    if (!(chunk instanceof Chunk)) {
      throw new Error('chunk must \'Chunk\' type');
    }
    if (chunk.error) {
      return ws.write('[ERROR] ' + chunk.error + '\n', enc, next);
    } else {
      return ws.write('' + chunk.data + '\n', enc, next);
    }
  };
  flush = function() {};
  stream = through2.obj(xf, flush);
  return stream;
};

fromUrl = function(option) {};

//// return fetch.FetchStream(url [, options]) -> Stream
fromMongo = async function(option) {
  var batchMode, client, collection, collectionName, connectUrl, db, dbName;
  ({connectUrl, dbName, collectionName, batchMode} = option);
  collection = null;
  client = (await MongoClient.connect(connectUrl));
  db = client.db(dbName);
  return db.collection(collectionName).find(option.findObj, option.projection);
};

fromMongo._meta = {
  type: 'source',
  name: 'fromMongo',
  option: [
    {
      optionName: 'connectUrl',
      type: 'string',
      default: 'mongodb://mongoadmin:secret@localhost:27017'
    },
    {
      optionName: 'dbName',
      type: 'string',
      default: 'local'
    },
    {
      optionName: 'collectionName',
      type: 'string',
      default: 'grants'
    },
    {
      optionName: 'query',
      type: 'object',
      default: {
        qty: {
          $gt: 25
        }
      }
    },
    {
      optionName: 'projection',
      type: 'object',
      default: {}
    }
  ]
};

intoMongo = function(option) {
  var batchMode, client, collection, collectionName, conP, connectUrl, dbName, flush, stream, xf;
  ({connectUrl, dbName, collectionName, batchMode} = option);
  collection = null;
  client = null;
  conP = new Promise((res, rej) => {
    var db;
    MongoClient.connect(connectUrl, (err, _client) => {});
    if (err) {
      return rej(err);
    }
    client = _client;
    db = _client.db(dbName);
    return res(db.collection(collectionName));
  });
  flush = function(cb) {
    console.log('close connection mongo');
    return cb();
  };
  xf = async function(chunk, encoding, next) {
    if (chunk.error) {
      return next(null, chunk);
    }
    collection = (await conP);
    if (batchMode) {
      return collection.insertMany(chunk.data, (err, result) => {
        if (!err) {
          return next(null, new chunk(result));
        } else {
          return next(null, new Chunk(err));
        }
      });
    } else {
      return collection.insert(chunk.data, (err, result) => {
        if (!err) {
          return next(null, new Chunk(result));
        } else {
          return next(null, new Chunk(err));
        }
      });
    }
  };
  // stream =  through2Concurrent.obj({highWaterMark:1,maxConcurrency: option.concurrent || 4}, xf, flush);
  stream = through2.obj(xf, flush);
  return stream;
};

intoMongo._meta = {
  type: 'transform',
  name: 'intoMongo',
  option: [
    {
      optionName: 'connectUrl',
      type: 'string',
      default: 'mongodb://mongoadmin:secret@localhost:27017'
    },
    {
      optionName: 'dbName',
      type: 'string',
      default: 'local'
    },
    {
      optionName: 'collectionName',
      type: 'string',
      default: 'etl_tmp_collection'
    }
  ]
};

module.exports = {toString, fromJSON, debug, tapIntoAgens, fromMysql, tapIntoMysql, intoMongo, fromMongo, take, batch, tap, tap2, throttle, map, reduce, moving, step, drop, sampling, counter, dataMap, aux, intoFile, toChunk, withErrorBypass, progress};
